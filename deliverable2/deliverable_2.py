# -*- coding: utf-8 -*-
"""deliverable 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KnWl-oyt1Q4NKIXxAnSoS1UspOqEXbI3
"""

! pip install serpapi google-search-results

from google.colab import userdata
SERPAPI_API_KEY = userdata.get('SERPAPI_API_KEY')

from serpapi import GoogleSearch
from typing import List, Dict, Any

def search_serpapi(query: str, api_key: str) -> List[Dict[str, Any]]:
    """
    Search using SerpAPI for the given query and return the results.

    :param query: The search query string.
    :param api_key: Your SerpAPI key.
    :return: A list of search results.
    :raises Exception: For any errors during the request.
    """
    try:
        search = GoogleSearch({
            "q": query,
            "location": "Austin, Texas, United States",
            "api_key": api_key
        })
        results = search.get_dict()
        return results.get("organic_results", [])
    except Exception as e:
        raise Exception(f"An error occurred: {e}")

# Example usage:
if __name__ == "__main__":
    query = "What are the biggest ethical concerns surrounding AI surveillance?"
    try:
        results = search_serpapi(query, SERPAPI_API_KEY)
        response = []
        for result in results:
            print(f"Title: {result['title']}")
            print(f"Link: {result['link']}\n")
            response.append({
                "title": result['title'],
                "link": result['link']
            })
    except Exception as e:
        print(f"Failed to fetch data: {e}")

!pip install requests beautifulsoup4 sentence-transformers transformers
import requests
from bs4 import BeautifulSoup

from sentence_transformers import SentenceTransformer, util
from transformers import pipeline

def rate_url_validity(user_query: str, url: str) -> dict:
    """
    Evaluates the validity of a given URL by: computing various metrics including
    domain trust, content relevance, fact-checking, bias, and citation scores.

    Args:
        user_query (str): The user's original query
        url (str): The URL to analyze

    Returns:
        dict: A dictionary containing scores for different validity aspects
    """

    # Step 1: Fetch Page Content
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        page_text = " ".join([p.text for p in soup.find_all("p")])
    except Exception as e:
        return {"error": f"Failed to fetch content: {str(e)}"}

    # Step 2: Domain Authority Check (Placeholder - Replace with Moz API)
    domain_trust = 60  # Replace with actual API call

    # Step 3: Content Relevance (Semantic Similarity)
    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
    similarity_score = util.pytorch_cos_sim(
        model.encode(user_query),
        model.encode(page_text)
    ).item() * 10

    # Step 4: Fact-Checking (Google Fact Check API)
    fact_check_score = check_facts(page_text)

    # Step 5: Bias Detection (Sentiment Analysis)
    sentiment_pipeline = pipeline(
        "text-classification",
        model="cardiffnlp/twitter-roberta-base-sentiment"
    )
    sentiment_result = sentiment_pipeline(page_text[:512])[0]
    bias_score = 100 if sentiment_result["label"] == "POSITIVE" else 50 if sentiment_result["label"] == "NEUTRAL" else 30

    # Step 6: Citation Check (Google Scholar via SerpAPI)
    citation_count = check_google_scholar(url)
    citation_score = min(citation_count * 10, 100)

    # Step 7: Compute Final Score
    final_score = (
        (0.3 * domain_trust) +
        (0.3 * similarity_score) +
        (0.2 * fact_check_score) +
        (0.1 * bias_score) +
        (0.1 * citation_score)
    )

    return {
        "Domain Trust": domain_trust,
        "Content Relevance": similarity_score,
        "Fact-Check Score": fact_check_score,
        "Bias Score": bias_score,
        "Citation Score": citation_score,
        "Final Validity Score": final_score
    }

def check_facts(text: str) -> int:
    """Check factual accuracy using Google Fact Check API"""
    api_url = f"https://toolbox.google.com/factcheck/api/v1/claimsearch?query={text[:200]}"
    try:
        response = requests.get(api_url)
        data = response.json()
        return 80 if "claims" in data and data["claims"] else 40
    except:
        return 50

def check_google_scholar(url: str) -> int:
    """Check Google Scholar citations using SerpAPI"""
    # Replace with your SerpAPI key
    serpapi_key = "SERPAPI_API_KEY"
    params = {"q": url, "engine": "google_scholar", "api_key": serpapi_key}
    try:
        response = requests.get("https://serpapi.com/search", params=params)
        data = response.json()
        return len(data.get("organic_results", []))
    except:
        return -1

if __name__ == "__main__":
    # Example usage
    queries = [
        ("What are the ethical challenges of AI-driven decision-making in healthcare?",
         "https://www.ajmc.com/view/ethical-considerations-for-ai-in-clinical-decision-making"),



        ("I have just been on an international flight, can I come back home to hold my 1-month-old newborn?",
         "https://www.mayoclinic.org/healthy-lifestyle/infant-and-toddler-health/expert-answers/air-travel-with-infant/faq-20058539")
    ]

    for query, url in queries:
        print(f"\nChecking URL: {url}")
        result = rate_url_validity(query, url)
        for k, v in result.items():
            print(f"{k}: {v:.2f}" if isinstance(v, float) else f"{k}: {v}")

